# Quick Start: Admin Evaluation Page

## ğŸš€ Installation

```bash
# 1. Install Python dependencies (if not already done)
cd agent_mgr
pip install -r requirements.txt

# 2. Start backend server
cd ../backend
npm run start:dev

# 3. Start frontend server (in new terminal)
cd ui
npm run dev
```

## ğŸ“ Access the Page

1. Open browser: `http://localhost:3000`
2. Login with admin account
3. Navigate to **Admin â†’ Evaluation** (in sidebar)

## ğŸ¯ What You'll See

```mermaid
flowchart TB
    Dashboard["Trajectory Evaluation Dashboard"]
    Toolbar["Toolbar: Dataset dropdown, Run Tests, Refresh"]
    Summary["Summary Cards: Total 5, Passed 4, Failed 1, Pass 80.0%"]
    PassRate["Overall Pass Rate: 80.0%"]
    Categories["Category Breakdown: CONTEXT 2/2, INTEGRATION 2/3"]
    Table["Test Results Table / wedding... context PASS 1.2s / venue... integration PASS 2.1s / greeting output FAIL 0.8s"]

    Dashboard --> Toolbar
    Dashboard --> Summary
    Dashboard --> PassRate
    Dashboard --> Categories
    Dashboard --> Table
```

## ğŸ¨ Color Guide

| Color | Meaning |
|-------|---------|
| ğŸŸ¢ Green | Passing test / Fast execution |
| ğŸ”µ Blue | Normal execution time |
| ğŸŸ¡ Yellow | Slow execution / Warning |
| ğŸ”´ Red | Failing test / Very slow / Critical issue |

## ğŸ“Š Test Scenarios

Currently available in `main_agent_scenarios`:

1. **wedding_info_query** (context)
   - Tests: Context agent information retrieval
   - Expected: PASS

2. **wedding_venue_location** (integration)
   - Tests: Context + Map integration
   - Expected: PASS

3. **wedding_directions** (integration)
   - Tests: Full agent orchestration
   - Expected: PASS

4. **simple_greeting** (output)
   - Tests: Output agent only
   - Expected: PASS

5. **complex_query** (integration)
   - Tests: Multi-tool coordination
   - Expected: MAY FAIL (needs improvement)

## ğŸ› ï¸ Common Actions

### Run All Tests
Click **"Run Tests"** button â†’ Wait ~30 seconds â†’ View results

### Refresh Results
Click **â†»** icon â†’ Loads cached results instantly

### Switch Dataset
Use dropdown â†’ Select different test suite â†’ Auto-loads results

### View Details
Each row shows:
- Test name and description
- Category badge
- Pass/Fail status
- Execution time
- Individual evaluation checkmarks

## âš ï¸ Troubleshooting

**"Failed to fetch" error**
â†’ Backend not running? Start: `cd backend && npm run start:dev`

**"Module not found" error**
â†’ Install dependencies: `cd agent_mgr && pip install -r requirements.txt`

**Tests all failing**
â†’ Check wedding-info.md exists in project root

**Page loading forever**
â†’ Check browser console (F12) for errors

## ğŸ“± Tips

- Results are **cached** after first run (instant reload)
- Tests run in **background** - don't close tab
- **Execution time** badges help identify performance issues
- **Category breakdown** shows which areas need work
- Use **text format** for debugging: `python3 run_evaluation.py --format text`

## ğŸ¯ Next Steps

1. âœ… Verify all tests pass
2. âœ… Check execution times are acceptable
3. âœ… Review agent responses for accuracy
4. âœ… Add more test scenarios as needed
5. âœ… Set up automated testing (optional)

---

Need more details? See `EVALUATION_PAGE_IMPLEMENTATION.md`
